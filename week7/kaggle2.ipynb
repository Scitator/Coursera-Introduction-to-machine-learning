{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/scitator/.conda/envs/python2/lib/python2.7/site-packages/pandas/computation/__init__.py:19: UserWarning: The installed version of numexpr 2.4.4 is not supported in pandas and will be not be used\n",
      "\n",
      "  UserWarning)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.calibration import calibration_curve\n",
    "from sklearn.cross_validation import KFold, StratifiedKFold, StratifiedShuffleSplit\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler, PolynomialFeatures\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import operator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def fillMissingData(df):\n",
    "    table = df.count()/df.shape[0]\n",
    "    i = 0\n",
    "    for x in table:\n",
    "        if x < 1.0: \n",
    "            if 'time' in df.columns.values[i]:\n",
    "                df[df.columns.values[i]].fillna(np.max(df[df.columns.values[i]])*1.1, inplace=True)\n",
    "            else:\n",
    "                df[df.columns.values[i]].fillna(df[df.columns.values[i]].mean(), inplace=True)\n",
    "        i = i + 1\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def prepareData(q):\n",
    "    df = pd.read_csv('./data/features_collection.csv', index_col='match_id')\n",
    "    y = df.radiant_win.values[:]\n",
    "    #df.fillna(600, inplace=True) # like 10 min after start\n",
    "    df = fillMissingData(df)\n",
    "    df.drop(['start_time', 'lobby_type', 'duration', 'radiant_win', 'tower_status_radiant', 'tower_status_dire', 'barracks_status_radiant', 'barracks_status_dire'], axis = 1, inplace=True)\n",
    "    \n",
    "    if 'gold' in q:\n",
    "        df['r_gold'] = df[['r1_gold', 'r2_gold', 'r3_gold', 'r4_gold', 'r5_gold']].mean(axis = 1)\n",
    "        df['d_gold'] = df[['d1_gold', 'd2_gold', 'd3_gold', 'd4_gold', 'd5_gold']].mean(axis = 1)\n",
    "    \n",
    "    if 'level' in q:\n",
    "        df['r_level'] = df[['r1_level', 'r2_level', 'r3_level', 'r4_level', 'r5_level']].mean(axis = 1)\n",
    "        df['d_level'] = df[['d1_level', 'd2_level', 'd3_level', 'd4_level', 'd5_level']].mean(axis = 1)\n",
    "\n",
    "    if 'xp' in q:\n",
    "        df['r_xp'] = df[['r1_xp', 'r2_xp', 'r3_xp', 'r4_xp', 'r5_xp']].mean(axis = 1)\n",
    "        df['d_xp'] = df[['d1_xp', 'd2_xp', 'd3_xp', 'd4_xp', 'd5_xp']].mean(axis = 1)\n",
    "\n",
    "    if 'lh' in q:\n",
    "        df['r_lh'] = df[['r1_lh', 'r2_lh', 'r3_lh', 'r4_lh', 'r5_lh']].mean(axis = 1)\n",
    "        df['d_lh'] = df[['d1_lh', 'd2_lh', 'd3_lh', 'd4_lh', 'd5_lh']].mean(axis = 1)\n",
    "\n",
    "    if 'kills' in q:\n",
    "        df['r_kills'] = df[['r1_kills', 'r2_kills', 'r3_kills', 'r4_kills', 'r5_kills']].mean(axis = 1)\n",
    "        df['d_kills'] = df[['d1_kills', 'd2_kills', 'd3_kills', 'd4_kills', 'd5_kills']].mean(axis = 1)\n",
    "\n",
    "    if 'deaths' in q:\n",
    "        df['r_deaths'] = df[['r1_deaths', 'r2_deaths', 'r3_deaths', 'r4_deaths', 'r5_deaths']].mean(axis = 1)\n",
    "        df['d_deaths'] = df[['d1_deaths', 'd2_deaths', 'd3_deaths', 'd4_deaths', 'd5_deaths']].mean(axis = 1)\n",
    "\n",
    "    if 'items' in q:\n",
    "        df['r_items'] = df[['r1_items', 'r2_items', 'r3_items', 'r4_items', 'r5_items']].mean(axis = 1)\n",
    "        df['d_items'] = df[['d1_items', 'd2_items', 'd3_items', 'd4_items', 'd5_items']].mean(axis = 1)\n",
    "        \n",
    "    for p in xrange(5):\n",
    "        df.drop(['r%d_gold' % (p+1),'r%d_level' % (p+1), 'r%d_xp' % (p+1), 'r%d_lh' % (p+1), \n",
    "                 'r%d_kills' % (p+1), 'r%d_deaths' % (p+1), 'r%d_items' % (p+1)], axis = 1, inplace=True)\n",
    "        df.drop(['d%d_gold' % (p+1),'d%d_level' % (p+1), 'd%d_xp' % (p+1), 'd%d_lh' % (p+1), \n",
    "                 'd%d_kills' % (p+1), 'd%d_deaths' % (p+1), 'd%d_items' % (p+1)], axis = 1, inplace=True)\n",
    "    df.drop(['first_blood_player1', 'first_blood_player2'], axis = 1, inplace=True)\n",
    "    return (df, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def binaryHeroVectorize(data):\n",
    "    df_heroes_list = pd.read_csv('./data/dictionaries/heroes.csv')\n",
    "    \n",
    "    dataPick = pd.DataFrame()\n",
    "    for name in df_heroes_list['name']:\n",
    "        dataPick[name] = np.zeros(data.shape[0])\n",
    "    dataPick.index = data.index\n",
    "\n",
    "    for i, match_id in enumerate(data.index):\n",
    "        for p in xrange(5):\n",
    "            heroName = df_heroes_list['name'][np.where(df_heroes_list['id'].values[:] == data.ix[match_id, 'r%d_hero' % (p+1)])[0][0]]\n",
    "            dataPick.ix[match_id, heroName] = dataPick.ix[match_id, heroName] + 1\n",
    "            heroName = df_heroes_list['name'][np.where(df_heroes_list['id'].values[:] == data.ix[match_id, 'd%d_hero' % (p+1)])[0][0]]\n",
    "            dataPick.ix[match_id, heroName] = dataPick.ix[match_id, heroName] - 1\n",
    "    \n",
    "    res = data.drop(['r1_hero', 'r2_hero', 'r3_hero', 'r4_hero', 'r5_hero', 'd1_hero', 'd2_hero', 'd3_hero', 'd4_hero', 'd5_hero'], axis = 1)\n",
    "    \n",
    "    return res, dataPick"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def binaryItemVectorize(df):\n",
    "    df_items_list = pd.read_csv('./data/dictionaries/items.csv')\n",
    "    \n",
    "    dataPick = pd.DataFrame()\n",
    "    for name in df_items_list['name']:\n",
    "        dataPick[name] = np.zeros((df.shape[0]))\n",
    "    dataPick.index = df.index\n",
    "        \n",
    "    for i, match_id in enumerate(df.index):\n",
    "        for p in xrange(5):\n",
    "            for item_id in df.ix[match_id, 'r%d_items_collection' % (p+1)].replace(\"[\", \"\").replace(\"]\", \"\").replace(\" \", \"\").split(\",\"):\n",
    "                if isinstance( item_id, int ):\n",
    "                    itemName = df_items_list['name'][np.where(df_items_list['id'].values[:] == int(item_id))[0][0]]\n",
    "                    dataPick.ix[match_id, itemName] = dataPick.ix[match_id, itemName] + 1\n",
    "            for item_id in df.ix[match_id, 'd%d_items_collection' % (p+1)].replace(\"[\", \"\").replace(\"]\", \"\").split(\",\"):\n",
    "                if isinstance( item_id, int ):\n",
    "                    itemName = df_items_list['name'][np.where(df_items_list['id'].values[:] == int(item_id))[0][0]]\n",
    "                    dataPick.ix[match_id, itemName] = dataPick.ix[match_id, itemName] - 1    \n",
    "    \n",
    "    res = df.drop(['r1_items_collection', 'r2_items_collection', 'r3_items_collection', 'r4_items_collection', 'r5_items_collection', \n",
    "                     'd1_items_collection', 'd2_items_collection', 'd3_items_collection', 'd4_items_collection', 'd5_items_collection'], \n",
    "                        axis = 1)\n",
    "    \n",
    "    return res, dataPick"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def binaryAbilityVectorize(df):\n",
    "    df_abilities_list = pd.read_csv('./data/dictionaries/abilities.csv')\n",
    "\n",
    "    dataPick = pd.DataFrame()\n",
    "    for name in df_abilities_list['name']:\n",
    "        dataPick[name] = np.zeros((df.shape[0]))\n",
    "    dataPick.index = df.index\n",
    "        \n",
    "    for i, match_id in enumerate(df.index):\n",
    "        for p in xrange(5):\n",
    "            for item_id in df.ix[match_id, 'r%d_ability_upgrades_collection' % (p+1)].replace(\"[\", \"\").replace(\"]\", \"\").replace(\" \", \"\").split(\",\"):\n",
    "                if isinstance( item_id, int ):\n",
    "                    itemName = df_abilities_list['name'][np.where(df_abilities_list['id'].values[:] == int(item_id))[0][0]]\n",
    "                    dataPick.ix[match_id, itemName] = dataPick.ix[match_id, itemName] + 1\n",
    "            for item_id in df.ix[match_id, 'd%d_ability_upgrades_collection' % (p+1)].replace(\"[\", \"\").replace(\"]\", \"\").split(\",\"):\n",
    "                if isinstance( item_id, int ):\n",
    "                    itemName = df_abilities_list['name'][np.where(df_abilities_list['id'].values[:] == int(item_id))[0][0]]\n",
    "                    dataPick.ix[match_id, itemName] = dataPick.ix[match_id, itemName] - 1\n",
    "    \n",
    "    res = df.drop(['r1_ability_upgrades_collection', 'r2_ability_upgrades_collection',\n",
    "                     'r3_ability_upgrades_collection', 'r4_ability_upgrades_collection', \n",
    "                     'r5_ability_upgrades_collection', \n",
    "                     'd1_ability_upgrades_collection', 'd2_ability_upgrades_collection',\n",
    "                     'd3_ability_upgrades_collection', 'd4_ability_upgrades_collection',\n",
    "                     'd5_ability_upgrades_collection'], axis = 1)\n",
    "    \n",
    "    return res, dataPick"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# def prepareScaleData(q, scale=True):\n",
    "#     (data, train_labels) = prepareData(q)\n",
    "#     data2, data_heroes = binaryHeroVectorize(data)\n",
    "#     data3, data_items = binaryItemVectorize(data2)\n",
    "#     train_data, data_abilities = binaryAbilityVectorize(data3)\n",
    "#     y = train_labels\n",
    "# #     X = np.hstack((train_data.values[:,:], data_heroes))\n",
    "# #     X = np.hstack((X, data_items))\n",
    "# #     X = np.hstack((X, data_abilities))\n",
    "#     if scale:\n",
    "#         scaler = StandardScaler()\n",
    "#         X_scale = scaler.fit_transform(X)\n",
    "#         return (X_scale, y, scaler)\n",
    "#     else:\n",
    "#         return (X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def prepareDataframe(q):\n",
    "    data, train_labels = prepareData(q)\n",
    "    data2, data_heroes = binaryHeroVectorize(data)\n",
    "    data3, data_items = binaryItemVectorize(data2)\n",
    "    train_data, data_abilities = binaryAbilityVectorize(data3)\n",
    "    df_new = pd.concat([train_data, data_heroes, data_items, data_abilities], axis=1)\n",
    "    df_new['win'] = train_labels\n",
    "    return df_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "qFeatures = ['gold', 'level', 'xp', 'lh', 'kills', 'deaths', 'items']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# qqFeatures = []\n",
    "# from itertools import combinations\n",
    "# for i in range(7):\n",
    "#     for x in combinations(qFeatures, i+1):\n",
    "#         qqFeatures.append(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# qScore = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# currentQ = []\n",
    "# for q in qqFeatures:\n",
    "#     (X, y, scaler) = prepareScaleData(q)\n",
    "#     lr = LogisticRegression()\n",
    "#     gridLogistic = {'C': np.power(10.0, np.arange(-5, 6)), 'penalty':('l1', 'l2')}\n",
    "#     kf = KFold(X.shape[0], n_folds=5, shuffle=True, random_state=42)\n",
    "#     gs = GridSearchCV(lr, gridLogistic, scoring='roc_auc', cv=kf, verbose=0, n_jobs=2)\n",
    "#     gs.fit(X, y)\n",
    "#     for a in gs.grid_scores_:\n",
    "#         qScore[\"(\" + str(q) + \"; \" + str(a.parameters['C']) + \"; \" + str(a.parameters['penalty']) + \")\"] = a.mean_validation_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# qScoreSorted = sorted(qScore.items(), key=operator.itemgetter(1), reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# for x in qScoreSorted:\n",
    "#     print \"par:\", x[0], ', roc: ', x[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#new_dataframe = prepareDataframe(qFeatures)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#for i in range(9):\n",
    "#    new_dataframe[10000*i:10000*(i + 1)].to_csv('./data/new_df{}_Index.csv'.format(i), \n",
    "#                                                sep=',', \n",
    "#                                                encoding='utf-8', \n",
    "#                                                header=True, \n",
    "#                                                index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('./data/new_df0_Index.csv', index_col='match_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X = df.drop('win', axis=1).values\n",
    "y = df['win'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "poly = PolynomialFeatures(2,include_bias=False)\n",
    "X_poly = poly.fit_transform(X[:,:31])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# new_dataframe.to_csv('./data/new_dataframe_noIndex.csv', sep=',', encoding='utf-8', header=True, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# df_to_write.to_csv('./data/featureCsv.csv', sep=',', encoding='utf-8', header=False, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "score = {}\n",
    "lr = LogisticRegression()\n",
    "gridLogistic = {'C': np.power(10.0, np.arange(-5, 6)), 'penalty':('l1', 'l2'), 'fit_intercept':(True, False)}\n",
    "kf = StratifiedKFold(y, n_folds=5, shuffle=True, random_state=42)\n",
    "gs = GridSearchCV(lr, gridLogistic, scoring='roc_auc', cv=kf, verbose=0, n_jobs=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "gs.fit(np.hstack((X,X_poly)), y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# нужна лишь часть выборки, иначе - жесть"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
